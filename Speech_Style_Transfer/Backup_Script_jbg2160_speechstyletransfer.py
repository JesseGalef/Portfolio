# -*- coding: utf-8 -*-
"""jbg2160_SpeechStyleTransfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/157CErdbKP68nU86g5K645Czn5irG8gKL

# Speech Style Transfer
## Jesse Galef uni: jbg2160

**To run:  **

Open the Files tab in the sidebar to the left

Click upload, select all files in the provided audio folder
(The audio files can also be found at:
https://drive.google.com/drive/folders/10-PJABVj6p69yGLzD99SnmzqctfQFPVT)

From the Runtime menu, select 'Run All'

Click 'refresh' in the Files tab, and download any generated audio to listen. They have names Original, fromReflect, fromConv, and fromiVector followed by \_0 through \_4

Audio output from the final run can be found at:
https://drive.google.com/open?id=1b2rfXjqwFue5DQUeTtNzTrM8iS1JkQjj

### Overview:
1. **Audio files** are from libriSpeech (training) and Audible.com (target)
2. **SpeechSignal object**: Core class, with functions to:


*   Frame, window, and preEmphasize signal
*   Calculate autocorrelations
*   Extract frame-by-frame Linear Predictive Coefficients, Reflection Coefficients, and residual via the Levinson-Durbin Algorithm


3. **Utility functions:**


*   reflectionToLPC - Converts Reflection Coefficients back to LPCs after modification
*   lpcToSignal - Reconstructs audio signal from LPC and residual 
*   convertAudio - Takes a SpeechSignal object and a trained model to convert the signal to the target style.  Saves the output in a given filename.
*   getPathDist - Performs dynamic time warping to generate a path which best aligns two series. Was ultimately not used in this version of the project. 


4. **Models/Generators:**


*  Input: Reflection Coefficients
*  Get Deltas and Delta-Deltas of the input
*  Convolutional layers which determine how much to add to the input (residual layer)
*  Calculate the 'features' of the new, modified content signal coefficients
*  Calculate the 'features' of style signal + Delta & Delta-Deltas
*  Compare the gram matrix (cross correlation) of the content features to the gram matrix of style features


5. **Three Sets of Features Used:**


* Just the Reflection Coefficients with Delta + Delta-Deltas
* Reflection Coefficients with Delta & Delta-Deltas passed through a static, randomly initialized wide Conv layer
* Features extracted in a simple speaker classifier - a stand-in for the iVectors which would be possible with a more sophisticated toolset like Kaldi
"""

import numpy as np
import scipy.signal
# import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import os
import time

# !pip install soundfile

import soundfile

fileNames = []
cwd = os.getcwd()
for fileName in os.listdir(cwd):
  if fileName[-4:] == 'flac':
    if fileName[:10] != 'The Return':
      fileNames.append(cwd+'/'+fileName)

class speechSignal():
    def __init__(self, fileName, start=0, emphasis=.97,
                 average=False, frameSize=512, frameStep=256):
        
        self.fileName = fileName
        self.emphasis=emphasis
        self.average=average
        self.frameSize = frameSize
        self.frameStep = frameStep
        
        self.rawData, self.sampleRate = soundfile.read(fileName)

        self.signal = np.array(self.rawData, copy=True)[start * self.sampleRate:]
        self.length = len(self.signal) / float(self.sampleRate)

        if len(self.signal.shape) > 1:
            self.signal = self.signal[:,0]
            # The Rickman audio is in stereo
                    
    def applyPreEmphasis(self, signal, emphasis=.97, average=True):
        signalCopy = np.array(signal, copy=True)
        signalCopy[1:] = signalCopy[1:] - (signalCopy[:-1] * emphasis)
        # This is the same as scipy.signal.lfilter([1,-emphasis],1, signal)

        return signalCopy

    def windowFunction(self, size):
        # hann window
        return np.sin(np.pi * np.arange(size) / (size-1))**2
    
    def autocorrs(self, signalSection, Q):
        """
        Gets the Q autocorrelation coefficients of a signal
        section.  Is used in the Levinson-Durbin algorithm
        """
        padded = np.concatenate([signalSection, np.zeros([Q])])
        r = np.ndarray([Q+1])
        for j in range(0,Q+1):
            r[j] = signalSection.dot(padded[j:len(signalSection)+j])
        return r   
    
    def getLPCs(self, Q=12):
        """
        Follows the Levinson-Durbin algorithm to find the
        Linear Predictive Coefficients and reflection coefficients for each
        frame.
        
        Passes the signal through this LPC all-zero filter to get the 
        residual, which gets saved as the excitation for each point.
        
        """

        signal = self.applyPreEmphasis(self.signal, self.emphasis, self.average)
        self.nFrames = (len(signal) - self.frameSize) // self.frameStep
        # This can truncate the end of the signal, which is ok for this project
        
        window = self.windowFunction(self.frameSize)
        
        self.alphas = np.ndarray([self.nFrames, Q+1])
        self.reflections = np.ndarray([self.nFrames, Q+1])
        self.gains = np.ndarray([self.nFrames])
        self.excitation = np.zeros([len(signal)])
        
        for i in range(self.nFrames):
            idx = i*self.frameStep+np.arange(self.frameSize)
            framedSignal = window * signal[idx]

            r = self.autocorrs(framedSignal, Q=Q)
            e = np.zeros([Q+1])
            alpha = np.zeros([Q+1])
            prevAlpha = np.zeros([Q+1])
            e[0] = r[0]
            alpha[0] = -1
            k = [1]
            for q in range(1,Q+1):
                newk = r[q]
                for j in range(1, q):
                    newk -= alpha[j]*r[q-j]
                newk = newk / e[q-1]
                alpha[q] = newk
                prevAlpha[:q] = alpha[:q] 
                k.append(newk)
                for j in range(1,q):
                    alpha[j] = alpha[j] - newk*prevAlpha[q-j]
                e[q] = (1.- newk**2)*e[q-1]

                
            self.alphas[i] = -alpha
            self.reflections[i] = k
            # putting the signal back through the filter in reverse
            # (as all-zero) will give us the residual
            resid = scipy.signal.lfilter(self.alphas[i], 1, framedSignal) 
            self.gains[i] = (resid ** 2).mean() ** .5
            self.excitation[idx] += resid / self.gains[i]

"""## Load Audio: 
### Alan Rickman reading The Return of the Native, and
### 1000 random 5-second clips from a selected LibriSpeech audio speaker

Calculate the LPC, Reflection coefficients, and residual excitation for each
(Make sure to adjust the frame size for the Rickman audio, which has a higher sample rate)
"""

# Take 1000 random 5-second clips
np.random.seed(42)
Q = 12
clipLength = 5
X = []
while len(X) < 1000:
    fName = np.random.choice(fileNames)
    newSignal = speechSignal(fName, emphasis=.97)
    if newSignal.length > clipLength:
        startSample = np.random.choice(range(len(newSignal.signal)-(clipLength*newSignal.sampleRate)))
        newSignal.signal = newSignal.signal[startSample: startSample + (clipLength*newSignal.sampleRate)]
        newSignal.getLPCs(Q)
        X.append(newSignal)

fileName = cwd+"/The Return of the Native.flac"
rickman = speechSignal(fileName, start=30, emphasis=.97)
rickman.frameSize = int((512. / X[0].sampleRate) * rickman.sampleRate)
rickman.frameStep = rickman.frameSize // 2
rickman.getLPCs(Q)

"""## Utility Functions for Processing Signal

### This is the remains of an effort to use dynamic time warping to align excitation signals... it seemed promising but didn't work in time
"""

# def dist(a,b):
#    # a and b are each list-like of length Q
#     return sum((a - b)**2)

# def getDistPath(a,b, distFunction = dist, shiftPenalty=.2):
    
#     backsteps = np.array([[0,-1],[-1,0],[-1,-1]])
#     m = np.ones([len(a)+1, len(b)+1]) * float('inf') 
#     #additional row and col for left/top border
#     m[0][0] = 0.0
#     p = np.zeros_like(m)
#     p[0] = 0
#     p[:,0] = 1
#     p[0,0] = -1
#     for row in range(1,len(a)+1):
#         for col in range(1,len(b)+1):
#             right = m[row,col-1] + shiftPenalty
#             down = m[row-1,col] + shiftPenalty
#             diag = m[row-1,col-1] + distFunction(a[row-1], b[col-1])
#             m[row,col] = min([right, down, diag])
#             p[row,col] = np.argmin([right, down, diag])
#     traceRow, traceCol = len(a), len(b)
#     trace = []
#     while [traceRow, traceCol] != [0,0]:
#         trace = [[traceRow, traceCol]]+trace
#         shift = backsteps[int(p[traceRow, traceCol])]
#         traceRow += shift[0]
#         traceCol += shift[1]
#     trace = [[traceRow, traceCol]]+trace    
#     return m[-1,-1], trace

def reflectionToLPC(k):
    """
    Reconstructs the Linear Predictive Coefficients from
    the reflection coefficients, k
    following the algorithm at
    https://www.mathworks.com/help/dsp/ref/lpctofromrc.html
    
    k is a list of coefficients of length Q+1, always starting with 1
    Returns a list of LPC of length Q+1, with the first value as 1
    """

    Q = len(k) -1
    lpcMatrix = np.zeros([Q+1, Q+1])
    lpcMatrix[:,0] = 1

    for p in range(Q):
        for m in range(1, p+1):
            lpcMatrix[p+1,m] = lpcMatrix[p,m] - k[p+1]*lpcMatrix[p,p+1-m]
        lpcMatrix[p+1,p+1] = -k[p+1]
    return lpcMatrix[-1]

def lpcToSignal(alphas, gains, excitation, frameSize=512, 
                frameStep=256, emphasis = .95):
    """
    Reconstructs the signal from a set of Linear Predictive Coefficients
    for each frame.
    
    Generally follows Prof. Dan Ellis' structure:
    https://github.com/dpwe/elene4896/blob/master/L06/lpc.py
    
    Passes a given excitation signal (the residual from encoding)
    through the LPC for each frame as an all-pole filter. Since the frames
    overlap, apply a hanning window to each new output before adding it
    to the new signal array.
    
    Lastly, reverse the pre-emphasis filter we applied, by using the same
    values as the denominator rather than the numerator.
    
    """
    nPoints = len(excitation)
    newSignal = np.zeros([nPoints])
    nFrames = alphas.shape[0]

    for i in range(nFrames):
        idx = (i*frameStep) + np.arange(frameSize)
        frameExcitation = excitation[idx]

        inverseResidual = scipy.signal.lfilter([1], alphas[i], frameExcitation)
        newSignal[idx] += np.hanning(frameSize) * inverseResidual * gains[i]
    newSignal = scipy.signal.lfilter([1], [1, -emphasis], newSignal)
    return newSignal

def convertAudio(X, model, fileName):
  content = X.reflections[:,1:]
  convertedRef = model.predict(content[np.newaxis,...])[0]
  convertedRef = np.concatenate([np.ones([X.nFrames,1]), convertedRef], -1)
  newmax = abs(convertedRef).max()
  if newmax > 1:
    convertedRef /= newmax
  
  newlpc = np.zeros_like(convertedRef)
  for i in range(X.nFrames):
    newlpc[i] = reflectionToLPC(convertedRef[i])
  
  synth = lpcToSignal(newlpc, X.gains, X.excitation, emphasis=X.emphasis)
  soundfile.write(fileName, synth, X.sampleRate)
  return synth, newlpc

"""# Build Modification Neural Nets

## Prepare the common elements:



*   Layer to add deltas & delta-deltas
*   Layer to get the gram matrix from features
*   Create generator object to provide batches of input in the proper format for each model
*   Create visualizeModifications function to look at how the model changes the content's reflection coefficients (to notice if anything is obviously wrong)
*   Set common parameters for the neural net architecture: number of convolutional filters and kernel size for each layer
"""

from keras.layers import Input, Conv2D, Conv1D, Dense, Flatten, Reshape, Add
from keras.layers import Lambda, Concatenate, Subtract
from keras.layers import MaxPool1D, AveragePooling1D
from keras.layers import UpSampling1D, add, Lambda
from keras.layers import TimeDistributed, Dropout
from keras.layers import Activation
from keras.models import Model

import keras.backend as K
import tensorflow as tf

def addDeltas(X):
    """ 
    Appends the deltas and deltaDeltas to X
    Pads with zeros as the first frame
    
    X is a tensor of shape [nBatch, nFrames, nFeatures]
    returns tensor iof shape [nBatch, nFrames, nFeatures*3]
    """
    deltas = tf.concat([tf.zeros_like(X[:,0:1,:]), X[:,1:,:] - X[:,:-1,:]], 1)
    deltaDeltas = tf.concat([tf.zeros_like(deltas[:,0:1,:]),
                             deltas[:,1:,:] - deltas[:,:-1,:]], 1)    
    return tf.concat([X, deltas, deltaDeltas], 2)


deltaLayer = Lambda(lambda x: addDeltas(x))
gramLayer = Lambda(lambda x: K.batch_dot(tf.transpose(x, [0,2,1]), x))

def generator(content, style, zerosShape, batchSize):
  """
  Generates batches of 
  [content reflection coefficients, style reflection coefficients], zeros
  
  in the correct sizes to be trained in the models
  
  content    - list of content SpeechSignal objects
  style      - single speechSignal object
  zerosShape - (usually square) matrix for the feature gram matrix
  batchSize  - int for how many (content, style), zeros pairs to return
  """
  while True:
      contentBatch = np.random.choice(range(len(content)),
                                      size=batchSize, replace=True)
      contentFeatures = []
      for c in contentBatch:
        # Remove the all-ones first coefficient
        cRef = content[c].reflections[:,1:]
        contentFeatures.append(cRef)

      nContentFrames = content[0].reflections.shape[0]
      nStyleFrames = style.reflections.shape[0]

      styleFeatures = []
      for i in range(batchSize):
        start = np.random.choice(nStyleFrames-nContentFrames)
        sRef = style.reflections[start + np.arange(nContentFrames), 1:]
        styleFeatures.append(sRef)


      zerosBatch = np.zeros([batchSize]+list(zerosShape))
      yield [np.array(contentFeatures), np.array(styleFeatures)], zerosBatch

# def visualizeModifications(model, X, title=None):
  # content = X.reflections[:,1:]
  # modified = model.predict(content[np.newaxis,...])[0]

  # plt.figure(figsize=[15,3])
  # plt.subplot(1,3,1)
  # sns.heatmap(content.T)
  # plt.xticks([])
  # plt.title('Original Reflection Coefficients')
  # plt.subplot(1,3,2)
  # sns.heatmap(content.T - modified.T)
  # plt.xticks([])
  # plt.title('Modifications')
  # plt.subplot(1,3,3)
  # sns.heatmap(modified.T)
  # plt.xticks([])
  # plt.title('New Reflection Coefficients')
  
  # if title != None:
  #   plt.suptitle(title)
  #   plt.tight_layout(rect=[0,0,1,.9])

convFilters = [32,64,64,128,128]
convKernels = [3,5,5,7,7]
nFrames = X[0].reflections.shape[0]
nCoefficients = X[0].reflections.shape[1] - 1

"""## First Model: Take gram matrix from Reflection Coefficients with Delta & Delta-Deltas"""

convLayers = []

contentInput = Input(shape=[nFrames, nCoefficients])
activity = deltaLayer(contentInput)

for j in range(len(convFilters)):
    convLayers.append(Conv1D(filters=convFilters[j],
                             kernel_size=convKernels[j],
                             padding='same', activation='tanh'))
    activity = convLayers[-1](activity)
    if j == 1:
        activity = MaxPool1D()(activity)
    if j % 2 == 1:
        activity = Dropout(.25)(activity)

activity = UpSampling1D()(activity)
resLayer = TimeDistributed(Dense(nCoefficients,
                                 kernel_initializer='zeros',
                                 bias_initializer='zeros'))

activity = resLayer(activity)
modified = add([activity, contentInput])

modifiedWithDelta = deltaLayer(modified)
contentGram = gramLayer(modifiedWithDelta)

styleInput = Input(shape=[nFrames, nCoefficients])
styleFeatures = deltaLayer(styleInput)
styleGram = gramLayer(styleFeatures)

gramMatrixDifference = Subtract()([styleGram, contentGram])

model = Model([contentInput, styleInput], gramMatrixDifference)
model.compile('adadelta', 'mse')

# zerosShape is nCoefficients * 3 to account for the Deltas and Delta-Deltas
gen = generator(X, rickman, 
                zerosShape=[nCoefficients*3, nCoefficients*3],
                batchSize=32)

model.fit_generator(gen, steps_per_epoch=16, epochs=250, verbose=0)

reflectionOutputModel = Model(contentInput, modified)
reflectionOutputModel.compile('adadelta','mse')

"""### Visualize the results - looking at a signal's reflection coefficients before and after"""

# visualizeModifications(reflectionOutputModel, X[0], 'Using Only Reflection Coefficients & Derivatives')

"""## Model 2: Pass RC with delta and delta-deltas through a static wide convolutional layer to get features"""

nFeatures = 1024

convLayers = []

convContentInput = Input(shape=[nFrames, nCoefficients])

activity = deltaLayer(convContentInput)

for j in range(len(convFilters)):
    convLayers.append(Conv1D(filters=convFilters[j],
                             kernel_size=convKernels[j],
                             padding='same', activation='tanh'))
    activity = convLayers[-1](activity)
    if j == 1:
        activity = MaxPool1D()(activity)
    if j % 2 == 1:
        activity = Dropout(.25)(activity)

activity = UpSampling1D()(activity)
resLayer = TimeDistributed(Dense(nCoefficients,
                                 kernel_initializer='zeros',
                                 bias_initializer='zeros'))
activity = resLayer(activity)
modified = add([activity, convContentInput])


featureLayer = Conv1D(filters=nFeatures, kernel_size=[5], padding='same')
featureLayer.trainable=False

modifiedWithDelta = deltaLayer(modified)
contentFeatures = featureLayer(modifiedWithDelta)


contentGram = gramLayer(contentFeatures)

styleInput = Input(shape=[nFrames, nCoefficients])
styleWithDeltas = deltaLayer(styleInput)
styleFeatures = featureLayer(styleWithDeltas)
styleGram = gramLayer(styleFeatures)

gramMatrixDifference = Subtract()([styleGram, contentGram])

convModel = Model([convContentInput, styleInput], gramMatrixDifference)
convModel.compile('adadelta', 'mse')

convLayerGen = generator(X, rickman, 
                zerosShape=[nFeatures, nFeatures],
                batchSize=32)

convModel.fit_generator(convLayerGen, steps_per_epoch=16, epochs=250, verbose=0)

convOutputModel = Model(convContentInput, modified)
convOutputModel.compile('adadelta','mse')

"""### Visualize the results of the Conv-based feature model"""

# visualizeModifications(convOutputModel, X[0], 'Using Static Conv. Filters')

"""## Model 3: Create faux i-vectors with speaker classification model

Since this doesn't have access to the transcript alignment and UBM from Kaldi, we need a workaround for iVectors.  To get them, we train a simple model to classify the two speakers and extract features from a middle layer.  This is a very rudimentary version of the initial Gatys et. al style transfer idea.

### Build a simple classifier to identify whether the speaker of an utterance is the LibriSpeech speaker or Alan Rickman.

After training, lock in the weights for the first 3 convolutional layers - these will get the faux i-Vectors from the content and style reflection coefficients in the next model
"""

nIVectorFeatures = 64

inputFeatures = Input(shape=[nFrames, Q])
activity = inputFeatures
activity = deltaLayer(activity)

conv1 = Conv1D(filters=32, kernel_size=[3], padding='same', activation='relu')
activity = conv1(activity)
conv2 = Conv1D(filters=32, kernel_size=[3], padding='same', activation='relu')
activity = conv2(activity)
activity = MaxPool1D()(activity)
conv3 = Conv1D(filters=nIVectorFeatures, kernel_size=[3], padding='same',
              activation='tanh')
conv3Features = conv3(activity)
activity = conv3Features
activity = Dropout(.25)(activity)

conv4 = Conv1D(filters=64, kernel_size=[3], padding='same', activation='relu')
activity = conv4(activity)
activity = MaxPool1D()(activity)
activity = Dropout(.25)(activity)
activity = Flatten()(activity)

dense1 = Dense(128, activation='relu')
activity = dense1(activity)
prediction = Dense(1, activation='sigmoid')(activity)

guessModel = Model(inputFeatures, prediction)
guessModel.compile('adadelta', 'binary_crossentropy')

# We need a different generator to provide input for this intermediary model

def guessGenerator(content, style, batchSize):
    """
    Generate batches of Reflection Coefficients from either the content
    or style, along with a binary identifier y
    """
    nContentFrames = content[0].reflections.shape[0]
    nStyleFrames = style.reflections.shape[0]

    while True:
        y = np.random.choice(2, size=batchSize, replace=True)
        X = []
        for i in range(batchSize):
            if y[i] == 0:
                c = np.random.choice(len(content))
                X.append(content[c].reflections[:,1:])
            else:
                start = np.random.choice(nStyleFrames-nContentFrames)
                X.append(style.reflections[start + np.arange(nContentFrames), 1:])
              
        yield np.array(X), y

gGen = guessGenerator(X, rickman, 32)
guessModel.fit_generator(gGen, steps_per_epoch=6, epochs=50, verbose=0)

# Set the faux-iVector feature layers now that they're "trained"
conv1.trainable=False
conv2.trainable=False
conv3.trainable=False

"""### Now that the faux i-vector extractor layers are trained and fixed, build the next style transfer model"""

convLayers = []


iVecContentInput = Input(shape=[nFrames, nCoefficients])

activity = deltaLayer(iVecContentInput)

for j in range(len(convFilters)):
    convLayers.append(Conv1D(filters=convFilters[j],
                             kernel_size=convKernels[j],
                             padding='same', activation='tanh'))
    activity = convLayers[-1](activity)
    if j == 1:
        activity = MaxPool1D()(activity)
    if j % 2 == 1:
        activity = Dropout(.25)(activity)

activity = UpSampling1D()(activity)
resLayer = TimeDistributed(Dense(nCoefficients,
                                 kernel_initializer='zeros',
                                 bias_initializer='zeros'))

activity = resLayer(activity)
activity = Reshape([nFrames, nCoefficients])(activity)

modified = add([activity, iVecContentInput])


modifiedWithDelta = deltaLayer(modified)
contentFeatures = conv3(MaxPool1D()(conv2(conv1(modifiedWithDelta))))

contentGram = gramLayer(contentFeatures)

styleInput = Input(shape=[nFrames, nCoefficients])
styleWithDeltas = deltaLayer(styleInput)
styleFeatures = conv3(MaxPool1D()(conv2(conv1(styleWithDeltas))))

styleGram = gramLayer(styleFeatures)

gramMatrixDifference = Subtract()([styleGram, contentGram])

iVectorModel = Model([iVecContentInput, styleInput], gramMatrixDifference)
iVectorModel.compile('adadelta', 'mse')

iVectorGen = generator(X, rickman, 
                zerosShape=[nIVectorFeatures, nIVectorFeatures],
                batchSize=32)

iVectorModel.fit_generator(iVectorGen, steps_per_epoch=16, epochs=250, verbose=0)

iVectorOutputModel = Model(iVecContentInput, modified)
iVectorOutputModel.compile('adadelta','mse')

# visualizeModifications(iVectorOutputModel, X[0], 'Using Faux i-Vectors')

"""# Generate 5 examples from each model"""

for i in range(5):
  Rsynth, Rnewlpc = convertAudio(X[i], reflectionOutputModel, 'fromReflect_'+str(i)+'.flac')
  Csynth, Cnewlpc = convertAudio(X[i], convOutputModel, 'fromConv_'+str(i)+'.flac')
  Vsynth, Vnewlpc = convertAudio(X[i], iVectorOutputModel, 'fromiVector_'+str(i)+'.flac')
  
  originalSynth = lpcToSignal(X[i].alphas, X[i].gains,
                              X[i].excitation, emphasis=X[i].emphasis)
  soundfile.write('Original_'+str(i)+'.flac', originalSynth, X[i].sampleRate)

"""## Additional Visualizations

### Look at how the faux i-Vector gram matrix changed from the original signal to the modified version
"""

iVectorGramModel = Model(iVecContentInput, contentGram)
iVectorGramModel.compile('adadelta','mse')

modifiedMatrixExample = iVectorGramModel.predict(X[0].reflections[:,1:][np.newaxis,...])[0]


iVectorGramModel = Model(styleInput, styleGram)
iVectorGramModel.compile('adadelta','mse')

contentMatrixExample = iVectorGramModel.predict(X[0].reflections[:,1:][np.newaxis,...])[0]

styleChunk = rickman.reflections[100:410,1:]
styleMatrixExample = iVectorGramModel.predict(styleChunk[np.newaxis,...])[0]

# vmin = -10
# vmax = 10

# plt.figure(figsize=[8,12])
# plt.subplot(3,2,1)
# sns.heatmap(contentMatrixExample, vmin=vmin, vmax=vmax)
# plt.xticks([])
# plt.yticks([])
# plt.title('Original Content Gram Matrix')

# plt.subplot(3,2,3)
# sns.heatmap(styleMatrixExample, vmin=vmin, vmax=vmax)
# plt.xticks([])
# plt.yticks([])
# plt.title('Style Gram Matrix')

# plt.subplot(3,2,5)
# sns.heatmap(contentMatrixExample - styleMatrixExample, vmin=vmin, vmax=vmax)
# plt.xticks([])
# plt.yticks([])
# plt.title('Difference', fontsize=18)


# plt.subplot(3,2,2)
# sns.heatmap(modifiedMatrixExample, vmin=vmin, vmax=vmax)
# plt.xticks([])
# plt.yticks([])
# plt.title('Modified Gram Matrix')

# plt.subplot(3,2,4)
# sns.heatmap(styleMatrixExample, vmin=vmin, vmax=vmax)
# plt.xticks([])
# plt.yticks([])
# plt.title('Style Gram Matrix')

# plt.subplot(3,2,6)
# sns.heatmap(modifiedMatrixExample - styleMatrixExample, vmin=vmin, vmax=vmax)
# plt.xticks([])
# plt.yticks([])
# plt.title('Difference After Modification', fontsize=18)

# plt.suptitle('Gram Matrix of Faux i-Vector Before and After Modification',
#             fontsize=20)
# plt.tight_layout(rect=[0,0,1,.95])

# """### Look at the three resynthesized audio signals from the same original version"""

# plt.figure(figsize=[8,8])
# plt.subplot(4,1,1)
# plt.plot(X[4].signal)
# plt.title('Original Signal')
# plt.ylim([-.2, .2])
# plt.subplot(4,1,2)
# plt.plot(Rsynth)
# plt.title('Reflection Coefficient Features')
# plt.ylim([-.2, .2])
# plt.subplot(4,1,3)
# plt.plot(Csynth)
# plt.title('Static Convolutional Features')
# plt.ylim([-.2, .2])
# plt.subplot(4,1,4)
# plt.plot(Vsynth)
# plt.title('Faux i-Vector Features')
# plt.ylim([-.2, .2])
# plt.tight_layout()

"""## All done!  Download the audio to listen

Click 'refresh' in the Files tab, and download any generated audio to listen. They have names Original, fromReflect, fromConv, and fromiVector followed by \_0 through \_4

They can also be found at https://drive.google.com/open?id=1b2rfXjqwFue5DQUeTtNzTrM8iS1JkQjj
"""

